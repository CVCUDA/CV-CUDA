# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE: One must import PyCuda driver first, before CVCUDA or VPF otherwise
# things may throw unexpected errors.
import pycuda.driver as cuda  # noqa: F401

import os
import sys
import json
import logging
import argparse
import subprocess
import multiprocessing as mp

common_dir = os.path.join(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
    "common",
    "python",
)
sys.path.insert(0, common_dir)

from perf_utils import maximize_clocks, reset_clocks  # noqa: E402


class NvtxRangeTimeInfo:
    """
    A class encapsulating the time information associated with an NVTX range.
    """

    def __init__(self, start_ms, end_ms):
        """
        :param start_ms: The start time in milliseconds for this NVTX range.
        :param end_ms: The end time in milliseconds for this NVTX range.
        """
        self.start_ms = start_ms
        self.end_ms = end_ms

    @property
    def duration_ms(self):
        """
        Returns the total execution time of this NVTX range.
        """
        return self.end_ms - self.start_ms


class NvtxRange:
    """
    A class representing an NVTX range with its timing information.
    """

    def __init__(
        self, flat_name, parent_range_id, cpu_time_info=None, gpu_time_info=None
    ):
        """
        :param flat_name: The flat name of the NVTX range - as represented in NSYS reports.
        :param parent_range_id: The integer range ID of the parent range of this NVTX range.
        :param cpu_time_info: An `NvtxRangeTimeInfo` holding the CPU timing information of this NVTX range.
        :param gpu_time_info: An `NvtxRangeTimeInfo` holding the GPU timing information of this NVTX range.
        """
        self.flat_name = flat_name
        self.parent_range_id = parent_range_id
        self.cpu_time_info = cpu_time_info
        self.gpu_time_info = gpu_time_info


def parse_nvtx_pushpop_trace_json(json_path):
    """
    Parses the nvtx_pushpop_trace JSON generated by NSYS and returns a dictionary
    keyed by process_id, thread_id and range_id. The values are various fields
    important to the benchmarking process.
    :param json_path: Full path to the nvtx_pushpop_trace.json file.
    """
    #
    # The nvtx_pushpop_trace JSON has the following structure. It is a list of
    # dictionaries.
    # e.g.
    #  [ {
    #   "Start (ns)": 2372801266,
    #   "End (ns)"  : 13528369268,
    #       ...
    #   },
    # ...
    # ]
    #
    # We will store the parsed data in the range_info dictionary. The overall
    # structure of the dictionary is:
    # range_info = {
    #       process_id : {
    #           thread_id : {
    #               range_id : NvtxRange(flat_name, parent_range_id, duration_ms)
    #           }
    #       }
    # }
    #
    #
    range_info = {}

    # Check if the file was empty or not. Empty file means no ops were recorded.
    if os.stat(json_path).st_size == 0:
        return range_info

    # Read the JSON.
    with open(json_path, "r") as f:
        json_data = json.loads(f.read())

    for row in json_data:
        # Grab the necessary values from the JSON file.
        flat_name = row["Name"]
        start_ns = float(row["Start (ns)"])
        end_ns = float(row["End (ns)"])
        range_id = row["RangeId"]
        parent_range_id = row["ParentId"]
        process_id = row["PID"]
        thread_id = row["TID"]

        # Process a bit. Conversion from nano to milliseconds.
        start_ms = round(start_ns / 10**6, 4)
        end_ms = round(end_ns / 10**6, 4)
        parent_range_id = None if parent_range_id == "None" else parent_range_id

        # Save it in our dictionary at the process id and thread id level.
        if process_id not in range_info:
            range_info[process_id] = {}
        if thread_id not in range_info[process_id]:
            range_info[process_id][thread_id] = {}

        # We wills save it using the Nvtx objects.
        cpu_time_info = NvtxRangeTimeInfo(start_ms, end_ms)
        nvtx_range = NvtxRange(flat_name, parent_range_id, cpu_time_info)

        range_info[process_id][thread_id][range_id] = nvtx_range

    return range_info


def parse_nvtx_gpu_proj_trace_json(json_path):
    """
    Parses the nvtx_gpu_proj_trace JSON generated by NSYS and returns a dictionary
    keyed by process_id, thread_id and range_id. The values are various fields
    important to the benchmarking process.
    :param json_path: Full path to the nvtx_gpu_proj_trace.json file.
    """

    #
    # The nvtx_gpu_proj_trace JSON has the following structure. It is a list of
    # dictionaries.
    # e.g.
    #  [ {
    #   "Projected Start (ns)": 2372801266,
    #   "Projected Duration (ns)"  : 13528369268,
    #       ...
    #   },
    # ...
    # ]
    #
    # We will store the parsed data in the range_info dictionary. The overall
    # structure of the dictionary is:
    # range_info = {
    #       process_id : {
    #           thread_id : {
    #               range_id : NvtxRange(flat_name, parent_range_id, cpu_duration_ms, gpu_duration_ms)
    #           }
    #       }
    # }
    #
    # NOTE: Even though this report returns the cpu_duration_ms and gpu_duration_ms, it will
    #       only do so for operations which had gpu_duration_ms > 0. For pure CPU operations,
    #       this report will not even return those ranges. That is the reason why we need to
    #       query the pushpop_trace report.
    #
    range_info = {}

    # Check if the file was empty or not. Empty file means no GPU ops were recorded.
    if os.stat(json_path).st_size == 0:
        return range_info

    # Read the JSON.
    with open(json_path, "r") as f:
        json_data = json.loads(f.read())

    for row in json_data:
        # Grab the necessary values from the JSON file.
        range_id = row["RangeId"]

        if range_id == "None":
            continue

        flat_name = row["Name"]
        cpu_start_ns = float(row["Orig Start (ns)"])
        cpu_duration_ns = float(row["Orig Duration (ns)"])
        cpu_end_ns = cpu_start_ns + cpu_duration_ns

        gpu_start_ns = float(row["Projected Start (ns)"])
        gpu_duration_ns = float(row["Projected Duration (ns)"])
        gpu_end_ns = gpu_start_ns + gpu_duration_ns

        parent_range_id = row["ParentId"]
        process_id = row["PID"]
        thread_id = row["TID"]

        # Process a bit. Conversion from nano to milliseconds.
        cpu_start_ms = round(cpu_start_ns / 10**6, 4)
        cpu_end_ms = round(cpu_end_ns / 10**6, 4)

        gpu_start_ms = round(gpu_start_ns / 10**6, 4)
        gpu_end_ms = round(gpu_end_ns / 10**6, 4)

        # Save it in our dictionary at the process id and thread id level.
        if process_id not in range_info:
            range_info[process_id] = {}
        if thread_id not in range_info[process_id]:
            range_info[process_id][thread_id] = {}

        # We wills save it using the Nvtx objects.
        cpu_time_info = NvtxRangeTimeInfo(cpu_start_ms, cpu_end_ms)
        gpu_time_info = NvtxRangeTimeInfo(gpu_start_ms, gpu_end_ms)
        nvtx_range = NvtxRange(flat_name, parent_range_id, cpu_time_info, gpu_time_info)

        range_info[process_id][thread_id][range_id] = nvtx_range

    return range_info


def expand_nvtx_range_names(range_info):
    """
    Converts a hierarchical NVTX range tree with parent-child relationship into a flat
    tree by adding the names of parent nodes in-front of all the child nodes.
    Hence, a tree like the following:
        root
            child_a
                   sub_child_a
            child_b
            child_c
                   sub_child_c

    becomes:
        root
        root.child_a
        root.child_a.sub_child_a
        root.child_b
        root.child_c
        root.child_c.sub_child-c

    :param range_info: The range_info dictionary returned by the parsing functions.
    """
    final_dict = {}

    # Loop over all the process from the range info dictionary.
    for process_id in range_info:
        if process_id not in final_dict:
            final_dict[process_id] = {}

        # Loop over all the threads from the range info dictionary.
        for thread_id in range_info[process_id]:
            if thread_id not in final_dict[process_id]:
                final_dict[process_id][thread_id] = {}

            # Loop over all the ranges from the range info dictionary.
            for range_id in range_info[process_id][thread_id]:

                # Fetch the range information.
                nvtx_range = range_info[process_id][thread_id][range_id]

                # There are two cases to consider:
                # 1. This was a root node (i.e no parent)
                # 2. This is not a root node (i.e has a parent)
                #
                my_parent_id = nvtx_range.parent_range_id
                if my_parent_id and my_parent_id != "None":
                    # This is not a root node. Get the information of its parent.
                    parent_nvtx_range = range_info[process_id][thread_id][my_parent_id]
                    # prepend parent's name in the child's name
                    new_name = os.path.join(
                        parent_nvtx_range.flat_name, nvtx_range.flat_name
                    )

                    # Most important to update our existing range info dictionary
                    # so any nested children will end up using the new, fully
                    # qualified name of this range.
                    nvtx_range.flat_name = new_name
                    range_info[process_id][thread_id][range_id] = nvtx_range

                    # And add it to our dictionary.
                    final_dict[process_id][thread_id][new_name] = nvtx_range

                else:
                    # This is a root node. Nothing else needs to be done other than
                    # simply adding this in our final dictionary.
                    final_dict[process_id][thread_id][nvtx_range.flat_name] = nvtx_range

    return final_dict


def merge_cpu_and_gpu_ranges(cpu_range_info, gpu_range_info):
    """
    Merges the CPU and GPU NVTX range information dictionaries into one such that it
    contains all the information. The keys will be flat expressions of NVTX range names
    and values will be a tuple of the CPU and GPU timings.

    NOTE: This is the function where the `NvtxRange` and `NvtxRangeTimeInfo` instances are
          used and converted into a couple of floating point duration values. In other
          words, even though the values of the `cpu_range_info` and `gpu_range_info`
          were `NvtxRange` objects, the mean can not be those (because mean of start
          and end times does not make sense.) Hence asking this function to calculate the
          mean means that only duration will be used and returned from those objects.

    :param cpu_range_info: The CPU range_info dictionary returned by the parsing functions.
    :param gpu_range_info: The GPU range_info dictionary returned by the parsing functions.
    """

    # Loop over all the keys in the cpu_range_info because it will have all the keys.
    # The gpu_range_info, may not have keys which were purely CPU code.
    all_ranges_info = {}
    for process_id in cpu_range_info:
        if process_id not in all_ranges_info:
            all_ranges_info[process_id] = {}

        for thread_id in cpu_range_info[process_id]:
            if thread_id not in all_ranges_info[process_id]:
                all_ranges_info[process_id][thread_id] = {}

            for range_name in cpu_range_info[process_id][thread_id]:
                nvtx_range = cpu_range_info[process_id][thread_id][range_name]

                gpu_time_info = NvtxRangeTimeInfo(0, 0)  # Initially it is set to zero.

                if process_id in gpu_range_info:
                    if thread_id in gpu_range_info[process_id]:
                        if range_name in gpu_range_info[process_id][thread_id]:
                            gpu_time_info = gpu_range_info[process_id][thread_id][
                                range_name
                            ].gpu_time_info

                nvtx_range.gpu_time_info = gpu_time_info

                all_ranges_info[process_id][thread_id][range_name] = nvtx_range

    return all_ranges_info


def calc_mean_ranges(all_range_info):
    """
    Calculates the mean of all NVTX ranges present in the all_range_info. Since NVTX ranges
    can be reported per process, per thread, we need to have a way to average those numbers.
    The mean here is computed by taking the average of all the numbers per process per thread.
    """
    mean_range_info = {}

    # Aggregate all the values in a list keyed by the range names.
    for process_id in all_range_info:
        for thread_id in all_range_info[process_id]:
            for range_name in all_range_info[process_id][thread_id]:
                if range_name not in mean_range_info:
                    mean_range_info[range_name] = (
                        [],
                        [],
                    )  # Mean lists for CPU and GPU time info

                cpu_time = all_range_info[process_id][thread_id][
                    range_name
                ].cpu_time_info.duration_ms
                gpu_time = all_range_info[process_id][thread_id][
                    range_name
                ].gpu_time_info.duration_ms

                mean_range_info[range_name][0].append(cpu_time)
                mean_range_info[range_name][1].append(gpu_time)

    # Replace the list with the mean value.
    for range_name in mean_range_info:
        if len(mean_range_info[range_name]):
            cpu_ranges_list = mean_range_info[range_name][0]
            gpu_ranges_list = mean_range_info[range_name][1]

            avg_cpu_time = round(sum(cpu_ranges_list) / len(cpu_ranges_list), 4)
            avg_gpu_time = round(sum(gpu_ranges_list) / len(gpu_ranges_list), 4)

            mean_range_info[range_name] = (avg_cpu_time, avg_gpu_time)
        else:
            mean_range_info[range_name] = (0, 0)

    return mean_range_info


class MeanDictInfo:
    """
    A small data structure to help track various stats over multiple dictionaries.
    For example, we can use to create one dictionary that represents the sum of
    many dictionaries. In that, this data structure can track the total value
    (i.e the sum) and how many items were added into making that sum (i.e len).
    """

    def __init__(self, value):
        self.value = value
        self.len = 0


def recurse_sum_dict(input_dict, target_dict):
    """
    Recursively sums up value of all keys of input_dict in another dictionary.
    This is useful for computing the total (or the mean eventually) of all the
    keys in a dictionary.
    This function uses and inserts a special object `MeanDictInfo` as values of
    the leaf nodes. That object is useful to track not just the sum total values
    but also how many items were summed up in there.
    :param input_dict: The dictionary that should be used as input.
    :param target_dict: The single dictionary in which all the sums should be gathered.
    """
    # Loop over all the keys in the input dictionary.
    for key in input_dict:
        # Check if the value is another dictionary.
        if isinstance(input_dict[key], dict):
            # Create this if our target_dict did not already have it.
            if key not in target_dict:
                target_dict[key] = {}

            # Recurse the same function again.
            recurse_sum_dict(input_dict[key], target_dict[key])

        # Check if the value is a list or tuple.
        elif isinstance(input_dict[key], list) or isinstance(input_dict[key], tuple):

            # Create this if our target_dict did not already have it. Instead
            # of saving just the sums, we will save both, the sum and the len
            # telling us how many numbers were summed up. We use the MeanDictInfo
            # object for this.
            if key not in target_dict:
                target_dict[key] = MeanDictInfo(value=[])
                for _ in range(len(input_dict[key])):
                    target_dict[key].value.append(0)

            for i in range(len(input_dict[key])):
                target_dict[key].value[i] += input_dict[key][i]

            # Increment the length.
            target_dict[key].len += 1

        # For anything else, we assume it was a number.
        else:
            if key not in target_dict:
                target_dict[key] = MeanDictInfo(value=0)

            target_dict[key].value += input_dict[key]
            target_dict[key].len += 1


def recurse_divide_dict(input_dict, divide_by=None):
    """
    Recursively divides the value of all keys of input_dict.
    The denominator can be a fixed value `divide_by` or it can be dynamically
    inferred from the length attributes of the MeanDictInfo object.
    :param input_dict: The dictionary that should be used as input.
    :param divide_by: Optional value to use in the denominator.
    """
    # Loop over all the keys in the input dictionary.
    for key in input_dict:
        # Check if the value is another dictionary.
        if isinstance(input_dict[key], dict):
            recurse_divide_dict(input_dict[key], divide_by)

        # Check if the value is a MeanDictInfo object.
        elif isinstance(input_dict[key], MeanDictInfo):
            if isinstance(input_dict[key].value, list) or isinstance(
                input_dict[key].value, tuple
            ):
                # Use the user given divide by if supplied or else use the len
                divide_by = divide_by if divide_by else input_dict[key].len

                for i in range(len(input_dict[key].value)):
                    input_dict[key].value[i] /= divide_by
                    input_dict[key].value[i] = round(input_dict[key].value[i], 4)
            else:
                divide_by = divide_by if divide_by else input_dict[key].len

                input_dict[key].value /= divide_by
                input_dict[key].value = round(input_dict[key].value, 4)

            # Remove the MeanDictInfo object and store the value directly.
            input_dict[key] = input_dict[key].value

        elif isinstance(input_dict[key], list) or isinstance(input_dict[key], tuple):
            if divide_by is None:
                raise ValueError(
                    "divide_by must not be None when the values of the dictionary are "
                    "not MeanDictInfo objects."
                )

            for i in range(len(input_dict[key].value)):
                input_dict[key][i] /= divide_by
                input_dict[key][i] = round(input_dict[key][i], 4)

        else:
            input_dict[key] /= divide_by
            input_dict[key] = round(input_dict[key], 4)


def unflatten_process_benchmark_dict(benchmark_dict, warmup_batches):
    """
    Un-flattens (i.e expands) the data present in benchmark_dict and also calculates
    additions numbers.
    """
    # This function needs to do a few different things. Here is the overall flow:
    #
    # 1. It has to expand the keys
    #       so 'run_sample/pipeline/batch_0/preprocess.cvcuda' from NSYS json
    #       becomes the following nested dictionary:
    #       run_sample : {
    #           pipeline : {
    #               batch_0 : {
    #                   preprocess.cvcuda : {"cpu_time": 0, "gpu_time": 0}
    #               }
    #           }
    #       }
    #
    # 2. Then it has to compute total of CPU and GPU times by aggregating those
    #    numbers at each level. In doing so, it has to account for warm-up batches
    #    i.e. batches whose timings should not be counted towards the total.
    #       run_sample : {
    #           pipeline : {
    #               batch_0 : {
    #                   preprocess.cvcuda  : {cpu_time: 0, gpu_time: 0}
    #                   postprocess.cvcuda : {cpu_time: 0, gpu_time: 0}
    #                   cpu_time : 0.0
    #                   gpu_time : 0.0
    #               }
    #           }
    #       }
    #
    #
    # 3. It also has to compute those times per frame/item. For this to happen, it needs
    #    the batch size information (i.e. how many items/frames were inside a batch) and
    #    also the information on which keys were "inside" a batch and which were not.
    #    These two pieces of information is taken from the benchmark.json
    #         pipeline : {
    #               batch_0 : {
    #                   preprocess.cvcuda : {cpu_time: 0, gpu_time: 0}
    #                   postprocess.cvcuda : {cpu_time: 0, gpu_time: 0}
    #                   cpu_time : 0.0
    #                   gpu_time : 0.0
    #                   cpu_time_per_item: 0.0
    #                   gpu_time_per_item: 0.0
    #               }
    #           }
    #       }
    #
    # 4. Finally, it computes what we call as the mean values of the timings from all
    #    the batches. In other words, it computes how much range X would take on an
    #    average when it is averaged across all the batches. To do this, we again use
    #    the information present inside benchmark.json and apply basic recursion math.
    #

    unfltten_data_dict = {}  # This is where we will store un-flattened data for now.

    # Maintains the total time of all warm-up batches.
    # this is keyed by the batch level prefix and values will be the time.
    total_warmup_cpu_time = {}
    total_warmup_gpu_time = {}
    # Maintain a count of total number of frames processed with counting the warm-up.
    total_items = {}
    # Maintain a count of total number of frames processed without counting the warm-up.
    total_items_minus_warmup = {}
    # Maintain pointers to the batch level sub-dictionaries. The keys will still be
    # the batch level prefix and values will be the dictionary.
    batch_dicts = {}

    # Loop over all the paths stored as keys in the input dictionary.
    for path in benchmark_dict["data"]:
        # Split the path expression by /
        parts = path.split("/")
        # Maintain a pointer to the dictionary current being traversed, initially set
        # to the empty results dictionary.
        current_dict = unfltten_data_dict
        # Loop over all but the last part, last part will be set as a value.
        for p in parts[:-1]:
            # Add the key if not already added, with a blank dict as its value.
            if p not in current_dict:
                current_dict[p] = {}

            # Update the dict pointer with nested expression.
            current_dict = current_dict[p]

        # Once all the sub-dictionaries are created, we need to assign them the correct
        # value. We need to be careful here because our expressions may have
        # nested keys such as:
        # batch_0:
        #       pre_process:
        #       post_process:
        # In the example above, we will have 3 numbers (2 for stages and 1 for overall batch)

        cpu_time = benchmark_dict["data"][path][0]
        gpu_time = benchmark_dict["data"][path][1]

        if parts[-1] not in current_dict:
            current_dict[parts[-1]] = {}

        current_dict[parts[-1]]["cpu_time"] = cpu_time
        current_dict[parts[-1]]["gpu_time"] = gpu_time

        # Now we will check at which exact level this path sits.
        # There are 3 possibilities:
        # 1. Exactly at the batch level.
        # 2. Inside the batch.
        # 3. Outside of/above the batch.
        #
        # Based on its placement, it would receive different treatments.

        # Check if this was at the batch level.
        if path in benchmark_dict["batch_info"]:
            # We are exactly at a batch level.
            batch_idx, batch_size = benchmark_dict["batch_info"][path]
            # Also find out the batch level prefix. i.e. one level above.
            batch_level_prefix = os.path.dirname(path)
            batch_dicts[batch_level_prefix] = current_dict

            # Add total items
            current_dict[parts[-1]]["total_items"] = batch_size

            # Computer per item.
            if batch_size > 0:
                current_dict[parts[-1]]["cpu_time_per_item"] = round(
                    current_dict[parts[-1]]["cpu_time"] / batch_size, 4
                )

                current_dict[parts[-1]]["gpu_time_per_item"] = round(
                    current_dict[parts[-1]]["gpu_time"] / batch_size, 4
                )

            # Maintain global counts of various batch level stats
            # for example, counting the total items seen at this
            # batch level.
            if batch_level_prefix not in total_items:
                total_items[batch_level_prefix] = 0
                total_items_minus_warmup[batch_level_prefix] = 0
                total_warmup_cpu_time[batch_level_prefix] = 0
                total_warmup_gpu_time[batch_level_prefix] = 0

            # Add to the totals at this batch level.
            total_items[batch_level_prefix] += batch_size

            # Check if this batch was not in the warm-up period.
            # Batches from the front and end are ignored that fall under the warm-up period.
            if batch_size > 0:
                if (
                    batch_idx + 1 > warmup_batches
                    and (batch_idx + 1 + warmup_batches)
                    <= benchmark_dict["meta"]["total_batches"][batch_level_prefix]
                ):
                    # This is a non-warmup batch.
                    total_items_minus_warmup[batch_level_prefix] += batch_size

                else:
                    # This is a warm-up batch. Add its timings so that we can
                    # subtract it later from the totals.
                    total_warmup_cpu_time[batch_level_prefix] += current_dict[
                        parts[-1]
                    ]["cpu_time"]
                    total_warmup_gpu_time[batch_level_prefix] += current_dict[
                        parts[-1]
                    ]["gpu_time"]

        elif path in benchmark_dict["inside_batch_info"]:
            # We could be inside a batch. Nothing to do here. We won't have the total_items yet
            # at this level
            pass
        else:
            # We are one or more levels outside/above the batch level.
            # We will need to correctly pass the total items stats here.
            current_dict[parts[-1]]["cpu_time_per_item"] = 0
            current_dict[parts[-1]]["gpu_time_per_item"] = 0

            # For cases where we are one level above the batch level, we can directly use the
            # stats stored in our dictionaries. Path's value here will be equal to batch_level_prefix

            if path in total_items:
                # We are exactly one level outside/above the batch level.
                if total_items[path] > 0:
                    current_dict[parts[-1]]["cpu_time_per_item"] = round(
                        current_dict[parts[-1]]["cpu_time"] / total_items[path],
                        4,
                    )
                    current_dict[parts[-1]]["gpu_time_per_item"] = round(
                        current_dict[parts[-1]]["gpu_time"] / total_items[path],
                        4,
                    )
                current_dict[parts[-1]]["total_items"] = total_items[path]

            else:
                # We are more than one level outside/above the batch level.
                # We will need to report stats summing up all the nested batch levels.
                total_items_above_level = 0

                for k in total_items:
                    if k.startswith(path):
                        total_items_above_level += total_items[k]

                if total_items_above_level > 0:
                    current_dict[parts[-1]]["cpu_time_per_item"] = round(
                        current_dict[parts[-1]]["cpu_time"] / total_items_above_level,
                        4,
                    )
                    current_dict[parts[-1]]["gpu_time_per_item"] = round(
                        current_dict[parts[-1]]["gpu_time"] / total_items_above_level,
                        4,
                    )
                current_dict[parts[-1]]["total_items"] = total_items_above_level

    # Add warm-up related keys exactly at the batch level dictionaries.
    # Warm-up time is the time taken by the warm-up number of batches
    # at the beginning and at the end of the pipeline.
    # So for any warm-up batches > 0, we add up their run times and
    # subtract those from the total run time at the end.
    for batch_level_prefix in batch_dicts:
        batch_dict = batch_dicts[batch_level_prefix]

        batch_dict["cpu_time_minus_warmup"] = round(
            (batch_dict["cpu_time"] - total_warmup_cpu_time[batch_level_prefix]), 4
        )
        batch_dict["gpu_time_minus_warmup"] = round(
            (batch_dict["gpu_time"] - total_warmup_gpu_time[batch_level_prefix]), 4
        )

        batch_dict["cpu_time_minus_warmup_per_item"] = 0
        batch_dict["gpu_time_minus_warmup_per_item"] = 0

        if total_items_minus_warmup[batch_level_prefix] > 0:
            batch_dict["cpu_time_minus_warmup_per_item"] = round(
                batch_dict["cpu_time_minus_warmup"]
                / total_items_minus_warmup[batch_level_prefix],
                4,
            )
            batch_dict["gpu_time_minus_warmup_per_item"] = round(
                batch_dict["gpu_time_minus_warmup"]
                / total_items_minus_warmup[batch_level_prefix],
                4,
            )

        batch_dict["total_items_minus_warmup"] = total_items_minus_warmup[
            batch_level_prefix
        ]

    # The processing is over. So we assign the expanded version of data into the
    # original benchmark dictionary.
    benchmark_dict["data"] = unfltten_data_dict

    # Finally, process the batches to find out the mean batch timings.
    # i.e. how much did range X took on an average across all the batches.
    # Again, we will not use any batches that are warm-up batches in this calculation.
    # For this to happen, we need to rely on the batch_info keys. Those are the
    # markers telling us what constitutes as "inside a batch". Then we build a union
    # of all the keys inside the batch, sum it up and find out the average.
    #
    # NOTE: Although not required, it would be good idea that to name the ranges inside
    #       various batches the same name.
    #       i.e. decode of batch_0 and batch_1 both be called 'decode'.
    #       It won't be an issue if that is not the case. Just that the average value
    #       will be averaged on non-uniform number of samples.
    #       e.g. if there is a range that is only used during last 3 batches, its mean
    #            value will be a mean over 3 samples compared to a range which is used
    #            during all the batches. Our division logic takes care of properly
    #            dividing with the current count anyway.
    #
    mean_data = {}
    total_batches_used = 0
    for batch_range_name in benchmark_dict["batch_info"]:
        batch_idx, batch_size = benchmark_dict["batch_info"][batch_range_name]
        # Next, we find out the batch level prefix. This is the key in which
        # the batches are nested. One profiling session can have multiple levels
        # at which batches may be used.
        # e.g.
        # program_X:
        #   method_A:
        #       batch_1
        #       batch_2
        #   method_B:
        #       batch_1
        #       batch_2
        #
        # We need to find mean at these two levels (i.e. method_A and method_B)
        # in this case.
        # programA/method_A and program_A/method_B are the batch level prefix here.
        # We can easily get those by using the dirname method since those are like
        # the directory names in a path.
        batch_level_prefix = os.path.dirname(batch_range_name)

        if (
            batch_size > 0
            and batch_idx + 1 > warmup_batches
            and (batch_idx + 1 + warmup_batches)
            <= benchmark_dict["meta"]["total_batches"][batch_level_prefix]
        ):
            # Keep on updating the mean_data dictionary. This will
            # create a dictionary that is union of all the dicts of the batch level.
            nested_keys = batch_range_name.split("/")
            target_dict = benchmark_dict["data"]
            for k in nested_keys:
                target_dict = target_dict[k]

            # Need to recursively update the mean_data based on
            # the target_dict. We will sum the values up.
            if batch_level_prefix not in mean_data:
                mean_data[batch_level_prefix] = {}

            recurse_sum_dict(target_dict, mean_data[batch_level_prefix])
            total_batches_used += 1

    # Once all the sums are calculated, we need to divide by the length to figure
    # out the mean values.
    recurse_divide_dict(mean_data)
    benchmark_dict["mean_data"] = mean_data

    # Remove the batch_info and inside_batch_info keys as they are no longer needed.
    del benchmark_dict["batch_info"]
    del benchmark_dict["inside_batch_info"]


def benchmark_script(
    process_idx,
    device_id,
    output_dir,
    warmup_batches,
    script,
    args,
):
    """
    Main function responsible for running an arbitrary python script and benchmarking it.
    :param process_idx: The 0-based index of this process.
    :param device_id: The GPU device id to use.
    :param output_dir: The output directory to use to store artifacts.
    :param warmup_batches: The numbers of batches that should be ignored from benchmarking.
    :param script: The python script to execute.
    :param args: Any optional command line arguments that should be passed to the script.
    """

    # Make a copy of the environment variables and add our own env-vars to it.
    my_env = os.environ.copy()
    # Change the CUDA visible devices for this process.
    my_env["CUDA_VISIBLE_DEVICES"] = str(device_id)
    # Add the benchmark flag so that perf_utils knows that this is a benchmark run.
    my_env["BENCHMARK_PY"] = "1"

    # Set a path to store the SQLITE report created by NSYS.
    out_sqlite_path = os.path.join(output_dir, "perf_report")
    # Set a path to the benchmark.json created by the script.
    benchmark_json_path = os.path.join(output_dir, "benchmark.json")

    # Remove any existing benchmark.json files.
    if os.path.isfile(benchmark_json_path):
        os.remove(benchmark_json_path)

    # Setup the command that will launch nsys and ask it to benchmark the script
    # that we were interested in.
    nsys_root_path = "/opt/nvidia/nsight-systems/2023.2.1/"
    nsys_binary_path = os.path.join(nsys_root_path, "bin/nsys")
    nsys_reports_path = os.path.join(nsys_root_path, "target-linux-x64/reports")
    nsys_gpu_proj_trace_report_path = os.path.join(
        nsys_reports_path, "nvtx_gpu_proj_trace"
    )
    nsys_pushpop_trace_report_path = os.path.join(
        nsys_reports_path, "nvtx_pushpop_trace"
    )

    if not os.path.isfile(nsys_binary_path):
        raise ValueError(
            "Unable to locate nsys binary at %s. Make sure you have nsight-systems 2023.2.1 installed."
            % nsys_binary_path
        )

    cmd = [
        nsys_binary_path,
        "profile",
        "--export",
        "sqlite",
        "-o",
        out_sqlite_path,
        "--force-overwrite",
        "true",
        "--trace",
        "cuda,nvtx",
        "--trace-fork-before-exec=true",
        sys.executable,
        script,
        *args,
    ]
    # Start the sub-process and wait for its completion.
    subproc = subprocess.Popen(cmd, stdout=None, stderr=None, env=my_env)
    subproc.wait()

    # Check if the subprocess was completed successfully. Proceed further only if yes.
    if subproc.returncode:
        return subproc.returncode, output_dir

    # Also if the script actually ran in the benchmark mode using our own perf_utils, it
    # must have generated a benchmark.json file for this process. That file may not be
    # it its final form (i.e. may still contain zero as perf values) but at-least it must be present.
    if not os.path.isfile(benchmark_json_path):
        logging.error(
            "benchmark.json was not found for process: %d at: %s. "
            "Did the script forget to call CvCudaPerf.finalize()?"
            % (process_idx, benchmark_json_path)
        )
        return 1, output_dir

    # Open and read the benchmark.json.
    with open(benchmark_json_path, "r") as f:
        benchmark_dict = json.loads(f.read())

    # Second step is to generate a JSON file from the SQLITE database generated by nsys.
    # We do this using nsys's stat command. We run it and wait for its completion.
    cmd2 = [
        nsys_binary_path,
        "stats",
        "--force-overwrite",
        "true",
        "-r",
        "%s,%s" % (nsys_gpu_proj_trace_report_path, nsys_pushpop_trace_report_path),
        "-f",
        "json",
        "-o",
        out_sqlite_path,
        out_sqlite_path + ".sqlite",
    ]
    subproc = subprocess.Popen(cmd2, stdout=None, stderr=None, env=my_env)
    subproc.wait()

    if subproc.returncode:
        return subproc.returncode, output_dir

    # Third step is to parse, process and merge the 2 JSONs generated by nsys above
    # One JSON is for CPU push-pop times and the other is for the GPU times.
    cpu_range_info = parse_nvtx_pushpop_trace_json(
        os.path.join(output_dir, "perf_report_nvtx_pushpop_trace.json")
    )
    gpu_range_info = parse_nvtx_gpu_proj_trace_json(
        os.path.join(output_dir, "perf_report_nvtx_gpu_proj_trace.json")
    )
    # Process
    cpu_range_info = expand_nvtx_range_names(cpu_range_info)
    gpu_range_info = expand_nvtx_range_names(gpu_range_info)
    # Merge
    all_range_info = merge_cpu_and_gpu_ranges(cpu_range_info, gpu_range_info)
    # Calculate averages across processes/threads
    mean_ranges_info = calc_mean_ranges(all_range_info)
    # and save it.
    with open(os.path.join(output_dir, "perf_report_nvtx_mean.json"), "w") as f:
        f.write(json.dumps(mean_ranges_info, indent=4))

    # Final step is to pull the data from the all_range_info we generated above and fill
    # it in the benchmark_dict.
    for k in mean_ranges_info:
        # Prepare the key to look for by prepending the obj_name value to the key.
        # obj_name is the key that sits at the root level and NSYS does not know about it.
        prepended_key = os.path.join(benchmark_dict["meta"]["obj_name"], k)
        if prepended_key in benchmark_dict["data"]:
            benchmark_dict["data"][prepended_key] = mean_ranges_info[k]

    # Un-flatten the benchmark dictionary and compute additional stats such as
    # per batch and per frame numbers. We rely on our own data that was saved in the
    # batch info key when benchmark.json was first created by perf_utils. This calculation
    # is not done by NSYS.
    unflatten_process_benchmark_dict(benchmark_dict, warmup_batches)

    # Write the updated benchmark dictionary.
    with open(benchmark_json_path, "w") as f:
        f.write(json.dumps(benchmark_dict, indent=4))

    return 0, output_dir


def main():
    parser = argparse.ArgumentParser(
        "Performance benchmarking script for CV-CUDA samples.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "-np",
        "--num_processes",
        type=int,
        default=1,
        help="The number of processes to spawn.",
    )

    parser.add_argument(
        "-ng", "--num_gpus", type=int, default=1, help="The number of GPUs to use."
    )

    parser.add_argument(
        "-go",
        "--gpu_offset_id",
        type=int,
        default=0,
        help="Offset for the GPU ids, assuming the GPUs are stacked together in a multi-GPU node.",
    )

    parser.add_argument(
        "-ll",
        "--log_level",
        type=str,
        choices=["info", "error", "debug", "warning"],
        default="info",
        help="Sets the desired logging level. Affects the std-out printed by the "
        "sample when it is run.",
    )

    parser.add_argument(
        "-o",
        "--output_dir",
        default="/tmp",
        type=str,
        help="The folder where the output results should be stored.",
    )

    parser.add_argument(
        "-w",
        "--warmup_batches",
        type=int,
        default=1,
        help="Sets the number of batches that should be ignored from being counted in "
        "the totals of the performance benchmarking numbers. These many batches are ignored"
        " from the front and the end.",
    )

    parser.add_argument(
        "-m",
        "--maximize_clocks",
        action="store_true",
        help="Maximizes the GPU clocks and power limits before running the benchmark. "
        "Clocks are not maximized by default.",
    )

    parser.add_argument(
        "script",
        help="The script that you want to benchmark.",
    )

    parser.add_argument(
        "args",
        nargs=argparse.REMAINDER,
        help="Any command-line arguments that should be passed to the script being benchmarked.",
    )

    args = parser.parse_args()

    if not os.path.isfile(args.script):
        raise ValueError("Script file does not exist at: %s" % args.script)

    logging.basicConfig(
        format="[%(name)s:%(lineno)d] %(asctime)s %(levelname)-6s %(message)s",
        level=getattr(logging, args.log_level.upper()),
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    logger = logging.getLogger("benchmark.py")

    # Check and raise error if the output directory exists in the child process's args.
    if "-o" in args.args or "--output_dir" in args.args:
        raise ValueError(
            "The output directory must only be specified once for benchmark.py. "
            "Do not specify it in the command line arguments of the script to be benchmarked."
        )

    # Maximize the clocks.
    if args.maximize_clocks:
        (
            did_maximize_clocks,
            was_persistence_mode_on,
            current_power_limit,
        ) = maximize_clocks(logger)

    # Start multiple processes, per num_processes per num_gpus.
    pool = mp.Pool()
    results = []

    for gpu_idx in range(args.num_gpus):
        for process_idx in range(args.num_processes):
            # Since each the output of each process needs to be stored in a different directory,
            # we will create the directory based on the process index and the GPU index.
            proc_output_dir = os.path.join(
                args.output_dir, "proc_%d_gpu_%d" % (process_idx, gpu_idx)
            )
            if not os.path.exists(proc_output_dir):
                os.makedirs(proc_output_dir)

            # Supply additional command-line arguments to make sure each process
            # behaves correctly.
            proc_device_id = str(args.gpu_offset_id + gpu_idx)
            proc_args = args.args.copy()
            # The following will make sure that it inserts the additional args
            # only at the beginning of the list so that it doesn't interfere with a
            # potentially argparse.REMAINDER style arg present at the end.

            # Need to set this to 0 because once CUDA_VISIBLE_DEVICES is used,
            # the process won't be able to see other gpus
            proc_args[:0] = ["--device_id", "0"]
            proc_args[:0] = ["--output_dir", proc_output_dir]
            # Start the pool.
            result = pool.apply_async(
                benchmark_script,
                args=(
                    process_idx,
                    proc_device_id,
                    proc_output_dir,
                    args.warmup_batches,
                    args.script,
                    proc_args,
                ),
            )
            logger.info("Launched process: %d. gpu-idx: %d" % (process_idx, gpu_idx))
            results.append(result)

    # Close the pool and wait everything to finish.
    pool.close()
    pool.join()

    # Reset the clocks.
    if args.maximize_clocks:
        reset_clocks(
            logger,
            was_persistence_mode_on,
            current_power_limit,
        )
    else:
        logger.warning("Clocks were not maximized during this run.")

    # Now we need to calculate the average of all the perf-numbers.
    # This means average across all the processes that we had launched.
    # This can only be done if all processes finished without error.
    # So we will check that first and if that is the case, we will
    # read their benchmark.json data in a list to later calculate
    # the average.
    all_benchmark_data_dicts = []
    all_benchmark_mean_batch_dicts = []
    for r in results:
        # Grab the return result from the pool.
        proc_ret_code, proc_output_dir = r.get()
        if proc_ret_code:
            # Any non-zero return code mean the process failed.
            raise Exception(
                "Failed to execute process: %d on gpu: %d" % (process_idx, gpu_idx)
            )
        else:
            # Zero return code means success. Read the benchmark.json.
            with open(os.path.join(proc_output_dir, "benchmark.json"), "r") as f:
                benchmark_dict = json.loads(f.read())

            # Append to our list of data dict and mean_batch data dict
            all_benchmark_data_dicts.append(benchmark_dict["data"])
            all_benchmark_mean_batch_dicts.append(benchmark_dict["mean_data"])

    mean_all_batch_data = {}
    mean_data = {}

    # First recursively sum up all values from all dictionaries.
    for data_dict in all_benchmark_data_dicts:
        recurse_sum_dict(data_dict, mean_all_batch_data)
    # And then divide by the length to get the mean values.
    recurse_divide_dict(mean_all_batch_data)

    for mean_batch_dict in all_benchmark_mean_batch_dicts:
        recurse_sum_dict(mean_batch_dict, mean_data)
    # And then divide by the length to get the mean values.
    recurse_divide_dict(mean_data)

    mean_benchmark_data = {
        "mean_all_batches": mean_all_batch_data,
        "mean_data": mean_data,
        "meta": {"args": {}},
    }
    for arg in vars(args):
        mean_benchmark_data["meta"]["args"][arg] = getattr(args, arg)

    # Write it in a file.
    mean_benchmark_json_path = os.path.join(args.output_dir, "benchmark_mean.json")
    with open(mean_benchmark_json_path, "w") as f:
        f.write(json.dumps(mean_benchmark_data, indent=4))
        logger.info(
            "Benchmarking completed successfully. Results saved at: %s"
            % mean_benchmark_json_path
        )


if __name__ == "__main__":
    main()
